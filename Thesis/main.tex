\documentclass[withoutpreface,bwprint]{cumcmthesis}
\usepackage{bm}
\usepackage{tablefootnote}
\usepackage{longtable}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\title{\zihao{3}{\heiti{2023年第二届“钉钉杯”大学生 \\ 大数据挑战赛论文}}}
\tihao{A}
\baominghao{2023070519067}
\schoolname{武汉科技大学}
\membera{常璐瑶}
\memberb{杜睿}
\memberc{韦钧舰}
\supervisor{}
\yearinput{2023}
\monthinput{07}
\dayinput{15}

\begin{document}
\pagestyle{empty}
\maketitle

\medskip
\begin{tabularx}{0.9\textwidth}{lc}
    \zihao{4}{题 \qquad 目：}\hspace{1em} & \zihao{4}{\hspace{3em} 智能手机用户监测数据分析 \hspace{3em}} \\[-2pt]
    \cline{2-2}
\end{tabularx}

\newblock

\begin{abstract}

    经过数据探索，发现原始数据存在一定的错误、冗余及缺失。数据清洗后，尝试从原始数据（数值、类别、时间）中提炼统计特征，以便于问题一和问题二的解决。

    本节首先选取合适量化指标预处理，然后分别采用原型（K-Means++）、密度（DBSCAN）、层次（AGNES）三种聚类算法对用户进行聚类，遵循“肘部法则”选择合理的聚类数量K值；最后，根据聚类结果对不同类别的用户画像，分析不同群体用户的特征。

    cumcmthesis 是为全国大学生数学建模竞赛编写的\LaTeX{}模板, 旨在让大家专注于 论文的内容写作, 而不用花费过多精力在格式的定制和调整上.
    本手册是相应的参考, 其 中提供了一些环境和命令可以让模板的使用更为方便. 同时需要注意, 使用者需要有一 定的 \LaTeX{} 的使用经验,
    至少要会使用常用宏包的一些功能, 比如参考文献，数学公式，图片使用，列表环境等等. 例子文件参看 \texttt{example.tex}.

    \keywords{\TeX{}\quad  图片\quad   表格\quad  公式}
\end{abstract}

%目录
\tableofcontents
\newpage

\pagestyle{mainmatterstyle}
\setcounter{page}{1}
\section{问题重述}

\subsection{问题背景}

智能手机已成为现代社会人们生活不可或缺的一部分，其普及和发展给人们带来了巨大的生活便利和娱乐享受。近年中国智能手机市场品牌竞争进一步加剧，中国超越美国成为全球第一大智能手机市场。随着智能手机市场快速增长，智能手机用户群体愈发多样，智能手机软件满目琳琅，研究智能手机用户的行为模式和使用偏好对于理解用户需求、预测用户行为和优化产品与服务具有重要意义。通过对智能手机用户监测数据的分析，可以为智能手机制造商、软件开发者、广告商和营销人员等提供有益的信息及有价值的洞察，指导他们制定战略和决策，更好地贴合用户需求并提供更佳的用户体验。

\subsection{问题要求}

\begin{description}
    \item[问题一] 针对问题一，赛题要求(1)根据用户常用所属的20类APP的数据对用户聚类，(2)对不同类别的用户画像，分析不同群体用户的特征。
    \item[]
\end{description}

\newpage
\section{数据探索、预处理与特征提取}

\subsection{符号说明与数据概览}

原始数据集包含app类别辅助表格（$A$.$\text{app\_class.csv}$）与21天监测数据（$B^*$.$\text{dayxx.txt}$），来源、符号、意义及数据类型如下\cref{tab:001}~所示。

\begin{table}[!htbp]
    \caption{数据集原始特征}\label{tab:001} \centering
    \begin{tabular}{ccccc}
        \toprule[1.5pt]
        来源       & 符号            & 意义              & 类型   \\
        \midrule[1pt]
        $A\&B^*$ & $appid$       & 用户的id，唯一标识一名用户  & 类别变量 \\
        $A$      & $app\_class$  & 应用的id，唯一标识一个APP & 类别变量 \\
        $B^*$    & $app\_type$   & APP类型：系统自带、用户安装 & 类别变量 \\
        $B^*$    & $start\_day$  & 使用起始天，取值1-30    & 数值变量 \\
        $B^*$    & $start\_time$ & 使用起始时间          & 时间变量 \\
        $B^*$    & $end\_day$    & 使用结束天           & 数值变量 \\
        $B^*$    & $end\_time$   & 使用结束时间          & 时间变量 \\
        $B^*$    & $duration$    & 使用时长（秒）         & 数值变量 \\
        $B^*$    & $up\_flow$    & 上行流量            & 数值变量 \\
        $B^*$    & $down\_flow$  & 下行流量            & 数值变量 \\
        \bottomrule[1.5pt]
    \end{tabular}
\end{table}

将$B^*$与$A$进行“左连接”得到$B$（同时舍弃重复值），$app\_class$为a～t的代表$A$中20个常用类别（$B$含3931种）；$NaN$则代表所属类别未知的不常用APP（$B$含32506种）。$B$中，在类别已知的常用20类APP中，t类数量最多（1406），r类最少（41）。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.83\textwidth]{app_class_countplot_in_days}
    \caption{B中各类APP计数图}
    \label{fig:001}
\end{figure}

\subsection{数据探索之类别变量}

\subsubsection{单一变量}
\begin{table}[!htbp]
    \caption{类别变量统计描述（以day01为例）}\label{tab:002} \centering
    \begin{tabular}{llrll}
        \toprule[1.5pt]
               & $uid$                            & $appid$ & $app\_type$ & $app\_class$ \\
        \midrule[1pt]
        count  & 5335803                          & 5335803 & 5335803     & 5335803      \\
        unique & 35451                            & 11021   & 4           & 21           \\
        top    & A9E4AAC5B8E05D2A4E35E0D4F2994F37 & 3309    & usr         & NaN          \\
        freq   & 2629                             & 924309  & 2987468     & 2432606      \\
        \bottomrule[1.5pt]
    \end{tabular}
\end{table}

据\cref{tab:002}~，$app\_type$只有两类【系统预装、用户安装】，存在异常。通过数据探索，发现表格存在['sys', 'usr', '用户', '预装']四种取值，故将中文全部替换成英文。

$app\_class$有$21$类，这是因为在“左连接”操作时，将$NaN$也作为一种APP类型，这是由于此处数据缺失本身就代表一种资讯（小众APP），并非随机发生或人为故意。如果将$NaN$ 也视作一种$app\_class$，那么数据$B$ 不存在缺失值。

\subsubsection{天数变化}
另外，从21天的类型变量数据可以发现，每日活跃用户、APP、日志条数在各天都有所差异，如\cref{fig:002}~所示。

\begin{figure}[!htbp]
    \centering
    \begin{minipage}[c]{0.3\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{relplot_line_day_val_logs}
        \subcaption{日志条数}
        \label{fig:002-a}
    \end{minipage}
    \begin{minipage}[c]{0.3\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{relplot_line_day_val_users}
        \subcaption{活跃用户数}
        \label{fig:002-b}
    \end{minipage}
    \begin{minipage}[c]{0.3\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{relplot_line_day_val_apps}
        \subcaption{活跃APP数}
        \label{fig:002-c}
    \end{minipage}
    \caption{day01～day21类别变量的变化折线图}
    \label{fig:002}
\end{figure}

粗略观察\cref{fig:002-b}，活跃用户数在$7$和$21$存在明显波谷，似乎和“星期”有某种关联；对照\cref{fig:002-a}、\cref{fig:002-b}、\cref{fig:002-c}三表分析，似乎APP活跃情况（种类、请求）与天数有所关联，甚至可以猜测某些小众APP被某些特定用户群体所使用甚至是青睐。

\subsubsection{变量关联}

APP自身包含$appid$、$app\_class$以及$app\_type$属性，因此可以抽取这三列建立$C$。

\begin{table}[!htbp]
    \caption{APP统计描述}\label{tab:003} \centering
    \begin{tabular}{lrll}
        \toprule
               & $appid$ & $app\_type$ & $app\_class$ \\
        \midrule
        count  & 37276   & 37276       & 37276        \\
        unique & 36437   & 2           & 21           \\
        top    & 19582   & usr         & NaN          \\
        freq   & 2       & 34153       & 32958        \\
        \bottomrule
    \end{tabular}
\end{table}

据\cref{tab:003}~，数据探索发现有839个app多重类型，即对于某些用户而言，该软件为系统预装，对另一些而言，则为自行安装。这似乎表明，不同的用户在软件下载与安装层面，有可相互区分的行为特征。另外，21类APP的数量情况如\cref{fig:003}~所示。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.95\textwidth]{catplot_count_app_class}
    \caption{21类APP计数图}
    \label{fig:003}
\end{figure}

据\cref{fig:003}~，未分类的APP并不是小数目（有一部分为系统预装），$t$类在APP多样性表现上依然出众。此外，不同种类的APP就类型（预装、用户）而言相差较为悬殊，例如：$g$类，系统预装相对较多；其余类别，用户预装较为普遍，尤其是$d$和$r$类。

值得区分的是，在新建APP统计数据$C$中，计数图反映了各类APP的多样程度（市场垄断程度）。而在监测数据$B$中，某个（类）APP请求日志（行数）计数图则反映了该用户在使用各类APP时的活跃（点击）行为，这将在下一小节进行探索。

\subsection{数据探索之数值变量}

\subsubsection{单一变量}

\begin{table}[!htbp]
    \caption{数值变量统计描述（以day01为例）}\label{tab:004} \centering
    \begin{tabular}{lrrrrr}
        \toprule[1.5pt]
              & $start\_day$ & $end\_day$ & $duration$     & $up\_flow$      & $down\_flow$   \\
        \midrule[1pt]
        count & 5335803      & 5335803    & 5335803        & 5335803         & 5335803        \\
        mean  & 0.975107     & 1          & 2151.604772    & 607572.168995   & 158163.759549  \\
        std   & 16.843899    & 0          & 1455335.155631 & 11015502.975274 & 6538529.614936 \\
        min   & -16524       & 1          & 1              & 0               & 0              \\
        25\%  & 1            & 1          & 3              & 0               & 0              \\
        50\%  & 1            & 1          & 10             & 0               & 0              \\
        75\%  & 1            & 1          & 36             & 1278            & 1063           \\
        max   & 1            & 1          & 1427769883     & 3639473769      & 3292713011     \\
        \bottomrule[1.5pt]
    \end{tabular}
\end{table}

数据具体说明指出：“$start\_day$：使用起始天，取值1-30（注：第一天数据的头两行的使用起始天取值为0，说明是在这一天的前一天开始使用的）。”然而，\cref{tab:004}~显示其最小值为$-16542$，因此可以判断$start\_day$存在异常值；而这会直接导致$duration$、$up\_flow$、$down\_flow$偏差过大。因此，须要对这两列进行数据清洗，删除异常值。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.95\textwidth]{boxenplot_duration_day01}
    \caption{使用时长、上下行流量增强箱形图（以day01为例）}
    \label{fig:004}
\end{figure}

据统计，在第1～21天监测数据中，$99.98\%$的记录使用时长不超过$9158.02$。针对异常案例进行分析，例如，$uid=64B3E40461C56847F35DB46D55707EA4$用户：

\begin{table}[!htbp]
    \caption{异常案例}\label{tab:005} \centering
    \begin{tabular}{lllllrlrlrrr}
        \toprule[1.5pt]
        appid & app\_class & start\_day & start\_time & end\_day & end\_time & duration \\
        \midrule[1pt]
        4803  & a          & 19         & 00:52:46    & 19       & 07:47:59  & 24912    \\
        18478 & c          & 19         & 07:48:25    & 19       & 07:48:38  & 12       \\
        \.    & \.         & \.         & \.          & \.       & \.        & \.       \\
        6192  & NaN        & 19         & 20:46:11    & 19       & 20:46:29  & 18       \\
        3309  & f          & 19         & 23:14:49    & 19       & 23:16:38  & 109      \\
        \bottomrule[1.5pt]
    \end{tabular}
\end{table}

凌晨零时至早上七时的记录是不符合生活常态不可持续的，猜测是应用后台驻留、系统故障或用户因故未关闭应用。不过，是否存在异常行为可以作为一个新的特征。因此，本文对持续时间超过$9159$的认定为无效使用时长，不能真实反映用户的行为特征。

此外，以第一天为例，不同用户在$appid$个数、$duration$总时长、$up\_flow$、$down\_flow$总流量、日志行数层面有所差异，分布如\cref{fig:005}~所示。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.95\textwidth]{histplots_appid_duration_upflow_downflow_lines}
    \caption{直方图（使用APP数量、使用有效总时长、消耗上下行流量、日志行数）}
    \label{fig:005}
\end{figure}

可以观察到某些用户较依赖手机，日志数、使用时长、APP多样性、消耗流量偏多。

\subsubsection{天数变化}

探索发现，可为每名用户绘制使用APP总数/时长/流量/日志随天数变化的折线图，以展现用户在月（周）级别的变化趋势与独潜在规律，并据此将用户群归类。

\subsubsection{变量关联}

更细粒度地，对于每名用户/每天，可绘制其各类APP的个数/时长/流量/日志情况，以了解用户对不同类型APP的适用情况、青睐程度；更进一步，还可以绘制24小时各类APP使用情况，这样便于了解用户的作息、通勤、活跃时段等信息。

\subsection{直觉小结与用户量化}

\subsubsection{数据直觉}
概括来说，赛题数据$A$给出4000多个常用APP所属类别：这是$1:1$的数据；而赛题数据$B$则记录了每名用户（$uid$）每日每时使用各款APP（$appid$）的起始时间，使用时长，上下流量等信息：这是$1:N$的资料。可以使用统计量进行归纳。

具体而言，监测数据蕴藏大量用户行为特征，例如：
\begin{itemize}
    \item 用户一天使用多长时间的手机（可间接反映依赖程度、年龄）
    \item 用户平均多长时间看一次手机（可间接反映依赖程度、年龄、工作）
    \item 距离上一次上线，隔了几天（可间接反映依赖程度、年龄、工作）
    \item 在什么星期几的时间段最常用什么类型APP（可反映工作、生活）
    \item 用户早、晚各使用什么类APP（可反映工作、生活、作息）
    \item 用户周末、工作日各使用什么类APP（可反映工作、生活、作息）
    \item 用户最早什么时候开始使用手机、什么时候结束使用（可反映工作、作息）
    \item 用户最常用什么类型APP（可反映喜好）
    \item 哪一类APP使用最频繁、哪一类使用时长最多（可反映喜好）
    \item 用户一共安装了多少个APP（可反映保守程度、对多样性的接纳程度）
    \item 系统预装与自行安装的比例（可反映保守程度、对多样性的接纳程度）
    \item 用户的每月流量使用情况（可间接反映财富程度、年龄）
\end{itemize}

\subsubsection{用户模型}

用户量化是指将现实生活中的“用户实体”进行抽象，采用不同维度的量化指标建模，即将其视为$n$维空间的一个点，使用形如$X=[x_1, x_2, \cdots , x_n]$的数学符号表示。

基于对数据集的深入探索及理解，提出简易用户模型：

\begin{table}[!htbp]
    \caption{简易用户模型}\label{tab:006} \centering
    \begin{tabular}{lllllrlrlrrr}
        \toprule[1.5pt]
        符号             & 意义                               & 维度                   \\
        \midrule[1pt]
        $uid$          & 用户的唯一标识                          & $1$                  \\
        $DU_{d, h, c}$ & 该用户在第$d$天$h$时内使用$c$类APP时，投入的总计时长 & $d\times h \times c$ \\
        $UF_{d, h, c}$ & 该用户在第$d$天$h$时内使用$c$类APP时，消耗的上行流量 & $d\times h \times c$ \\
        $DF_{d, h, c}$ & 该用户在第$d$天$h$时内使用$c$类APP时，消耗的下行流量 & $d\times h \times c$ \\
        $NO_{d, h, c}$ & 该用户在第$d$天$h$时内使用$c$类APP时，记录的日志行数 & $d\times h \times c$ \\
        \bottomrule[1.5pt]
    \end{tabular}
\end{table}

\section{问题一：聚类分析与用户画像}

\subsection{特征工程与评价指标}

\subsubsection{特征选择与数据降维}

聚类指将数据样本对象划分成若干类（簇、标签）并尽可能的保证“类内紧凑”、“类间独立”\cite{数据仓库与数据挖掘}。不同的量化指标、不同的相似度量（距离定义），往往会带来迥异的聚类结果。一般来说，量化指标维度数目越多，算法运行时间越长、结论可解释性越弱。

关于用户画像的量化指标，陈\cite{陈纯}、成\cite{成雪}等人从\uwave{日均屏幕使用时间}切入；
武\cite{武慧娟}等人从\uwave{APP数量、阅读时间}等特征对阅读类APP使用人群进行聚类解读；
侯\cite{侯金凤}针对\uwave{每日手机使用时长}、\uwave{使用频次}、\uwave{使用偏好}等特征对用户进行建模；
韦\cite{韦磊}基于\uwave{安装数量}、\uwave{打开次数}、\uwave{使用时长}、\uwave{工作日使用时长}、\uwave{周末使用时长}构建用户特征。

首先，为避免数据泄漏，仅选择前7日各类APP总时长、频次、上下行流量作为量化特征，共计$20 \times 4=80$维；对于不同的量纲特征，分别扣除均值，除以标准差以进行数值标准化。Pearson相关热力\cref{fig:pearson}~显示，各类APP时长、频次、流量成弱正相关。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.91\textwidth]{heatmap_features}
    \caption{皮尔逊相关系数热力图}
    \label{fig:pearson}
\end{figure}

为增加数据易用性，降低计算开销，增强视觉理解，而后采用主成分分析对特征进行变换，并按方差排序表示各维度重要程度，如\cref{fig:bar-pca}~，选定阈值将维度压缩至$3$维。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.91\textwidth]{barplot_pca_features}
    \caption{PCA特征方差柱状图}
    \label{fig:bar-pca}
\end{figure}

\subsubsection{评价指标}

聚类“好坏”不存在绝对的客观的标准\cite{机器学习}；聚类数目设定是否“合理”也往往依赖人工先验知识\cite{高维数据的聚类分析}。聚类数目设定过低，划分粒度不够细腻；聚类数目设定过高，宏观结论的可解释性又受到限制。常用选择聚类数目方法是人为观察聚合系数折线图，大致估计最优聚类数量$K$。相关定义如下：

\begin{definition}
    \textbf{各簇畸变程度}：该簇重心与其内部成员位置距离的平方和；
    假设一共将$n$个样本划分到$K$个簇中，用$C_k$表示第$k$簇，该簇重心记为$u_k$，则第$k$簇的畸变程度为：$$\sum_{i\in C_k} |x_i-u_k|^2$$
    \label{def:001}
\end{definition}

\begin{definition}
    \textbf{聚合系数}：$$J=\sum_{k=1}^{K} \sum_{i \in C_k} |x_i-u_k|^2$$
    \label{def:002}
\end{definition}

此外，还有Calinski-Harabasz系数\cite{CH}、Davies-Bouldin指数\cite{DB}、Silhouette轮廓系数\cite{SC}可用于度量某些聚类目的下的结论性能。

\subsection{算法概述与K值选择}

注：本小节所使用算法及评价指标均采用scikit\_learn\cite{sklearn}实现。

\subsubsection{原型聚类：K-Means++}

K-Means是一种简单、高效的聚类算法，假设聚类结构能通过一组“原型”刻画，算法的主要思想是通过迭代过程把数据集划分为不同的类别，流程如\cref{fig:008}~。K-means++优化“初始化K个聚类中心”，要求初始的聚类中心之间的相互距离要尽可能的远，在“孤立点数据敏感性”方面优于K-Means算法。默认采用欧式距离、重心法进行相似度量。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.37\textwidth]{process_kmeans}
    \caption{KMeans算法流程图}
    \label{fig:008}
\end{figure}

将最大迭代次数设置为1000，选择K等于2～50绘制聚合系数与卡林斯基-哈拉巴斯指数折线图。根据\cref{fig:009}~，K值从2到13时，畸变程度变化最大；超过6畸变程度变化显著降低：因此根据肘部法则，可将聚类数量$K$设定为5；从来看，应将聚类数量设定为6以下。该结论符合卡林斯基-哈拉巴斯指数峰值，故将聚合数目设定为$5$。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.77\textwidth]{lineplot_Ks_inertias_CH.pdf}
    \caption{K-Means 聚合系数与卡林斯基-哈拉巴斯指数}
    \label{fig:009}
\end{figure}

\subsubsection{层次聚类：BIRCH+AGNES}

AGNES算法（Agglomerative
Nesting），以自底向上方式，不断重复合并，产生不同粒度（层次）的聚类结果，一般最终预设聚类数目为1。该算法默认采用“欧式距离”进行度量样本距离，采用“离差平方和”（ward
linkage）作为簇距离度量函数，可通过聚类谱系图（dendrogram）可视化，算法执行流程如下：

\IncMargin{1em}
\begin{algorithm} \SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up} \SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress} \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}

    \Input{样本集$D=\{ x_1, x_2, \ldots, x_m \}$； \\
        聚类簇距离度量函数$d$；\\
        聚类簇数$k$。}
    \Output{簇划分：$\mathcal{C}=\{ C_1, C_2, \ldots, C_k \}$ }
    \BlankLine

    \BlankLine
    \emph{\#先将每个样本视作一个初始簇构造}\;
    \emph{\#构造$M$个类，每个类仅包含一个样本}\;
    \For{$j = 1,2, \ldots, m$}{
    $C_j={x_j}$
    }

    \BlankLine
    \emph{\#两两计算距离}\;
    \For{$i = 1,2, \ldots, m$}{
    \For{$j = i+1, \ldots, m$}{
    $M_{i, j}=d(C_i, C_j)$\;
    $M_{j, i}=M_{i, j}$
    }
    }

    \BlankLine
    \emph{\#当前类个数大于预设簇数}\;
    \While{$q > k$}{

    合并距离最近的两个聚类簇$C_{i^*}= C_{i^*} \cup C_{j^*}$\;
    \For{$j=j^*+1, j=j^*+2, \ldots, q$}{
        将聚类簇$C_{j}$重编号为$C_{j-1}$
    }

    删除距离矩阵$M$的第$j*$行与第$j*$列\;
    \# 重新计算距离矩阵\;
    \For{$j=1,2, \ldots, q-1$}{
    $M_{i^*, j}=d(C_{i^*}, C_{j})$\;
    $M_{j, i^*}=M_{i^*, j}$
    }
    q=q-1
    }
    \caption{AGNES算法}
\end{algorithm}
\DecMargin{1em}

AGNES算法时间复杂度为立方级别，对于本题万级别用户而言，耗费时间过长。因此，本文首先采用BIRCH\cite{birch}聚类算法得到初步聚类质心，作为AGNES算法的输入。BIRCH算法综合了层次凝聚和迭代的重定位方法首先用自底向上的层次算法，然后用迭代的重定位来改进结果。它的主要思想是：扫描数据库，建立一个初始存放于内存中的聚类
特征树然后对聚类特征树的叶结点进行聚类\cite{birch2}。

针对本题，首先将前7日44253名用户采用BIRCH算法聚类获得2827枚质心，而后AGNES对质心进行聚类，并绘制6层谱系图如下。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.6\textwidth]{dendrogram}
    \caption{BIRCH+AGNES谱系图}
    \label{fig:010}
\end{figure}

\subsubsection{密度聚类：DBSCAN}

DBSCAN算法从样本密度的角度来考察样本之间的可连接性，要求聚类空间中的以$eps$为半径的邻域内所包含对象的数目不小于某一给定阈值$min\_samples$，并基于可连接样本不断扩展生长聚类簇以获得最终的聚类结果\cite{机器学习}。算法将数据样本点分为三类：

\begin{enumerate}
    \item 核心点：在半径$eps$内含有不少于$min\_samples$数目的点
    \item 边界点：在半径$eps$内点的数量小于$min\_samples$，但是落在核心点的邻域内
    \item 噪声点：既不是核心点也不是边界点的点
\end{enumerate}

DBSCAN不需要预先输入要划分的聚类个数，但是对$eps$、$min\_samples$参数敏感。记特征维度数目$N=3$、$K=2N-1$，按照以下经验值确定超参数\cite{eps1,eps2}：$min\_samples=2N=6$，将数据集各点与K-最近邻算法分类标签的距离排序，观察\cref{fig:011}~拐点$y$坐标确定$eps=2$。

运行结果如下，聚类数量为$7$，噪点用户$539$名。

\begin{figure}[!htbp]
    \centering
    \begin{minipage}[c]{0.49\textwidth}
        \centering
        \includegraphics[width=.62\textwidth]{relplot_knn_distances}
        \caption{数据集各点6-最近邻距离（排序）}
        \label{fig:011}
    \end{minipage}
    \begin{minipage}[c]{0.49\textwidth}
        \centering
        \includegraphics[width=.99\textwidth]{dbscan}
        \caption{Visual Studio运行结果截图}
        \label{fig:012}
    \end{minipage}
\end{figure}

\subsection{算法比较与用户画像}

\subsubsection{算法比较}

截取BIRCH+AGNES聚类数目为$6$时的聚类结果。先采用内部评价指标比较，再使用PCA降维特征绘制三维散点图，最后使用二维t-SNE\cite{tsne}可视化原始80维特征。

\begin{table}[!htbp]
    \caption{聚类算法内部评价指标}\label{tab:007} \centering
    \begin{tabular}{llrrr}
        \toprule[1.5pt]
          & $Method$    & $Calinski-Harabasz$            & $Davies-Bouldin$           & $Silhouette$               \\
        \midrule[1pt]
        0 & K-Means++   & \textit{\textbf{32187.076682}} & \textit{\textbf{0.766536}} & 0.511919                   \\
        1 & DBSCAN      & 2870.580217                    & 2.817892                   & \textit{\textbf{0.664019}} \\
        2 & BIRCH+AGNES & 24732.933816                   & 0.860360                   & 0.622801                   \\
        \bottomrule[1.5pt]
    \end{tabular}
\end{table}

\begin{figure}[!htbp]
    \centering
    \begin{minipage}[c]{0.25\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{3d_kmeans}
        \subcaption{K-Means++}
        \label{fig:013-a}
    \end{minipage}
    \begin{minipage}[c]{0.25\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{3d_dbscan}
        \subcaption{DBSCAN}
        \label{fig:013-b}
    \end{minipage}
    \begin{minipage}[c]{0.25\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{3d_brc_agn}
        \subcaption{BIRCH+AGNES}
        \label{fig:013-c}
    \end{minipage}
    \caption{PCA三维散点图}
    \label{fig:013}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \begin{minipage}[c]{0.27\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{2d_kmeans}
        \subcaption{K-Means++}
        \label{fig:014-a}
    \end{minipage}
    \begin{minipage}[c]{0.27\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{2d_dbscan}
        \subcaption{DBSCAN}
        \label{fig:014-b}
    \end{minipage}
    \begin{minipage}[c]{0.27\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth]{2d_brc_agn}
        \subcaption{BIRCH+AGNES}
        \label{fig:014-c}
    \end{minipage}
    \caption{t-SNE二维散点图}
    \label{fig:014}
\end{figure}

散点图显示DBSCAN聚类较为笼统，K-Means++与BIRCH+AGNES类间区隔更清晰；从\cref{tab:007}~看，K-Means++更优，故根据K-Means++聚类结果对各用户群体画像。

\subsubsection{用户画像}

根据聚类结果进行数据可视化，可以获得5号人群对20种APP使用时长排序：

$A$类APP：2号 $\approx$ 0号 $\ge$ 1号 $\approx$ 4号 $\ge$ 3号

$B$类APP：2号 $\approx$ 0号 $\ge$ 4号 $\ge$ 1号 $\ge$ 3号

$C$类APP：3号 $\approx$ 1号 $\ge$ 2号 $\approx$ 0号 $\ge$ 4号

$D$类APP：0号 $\ge$ 2号 $\ge$ 1号 $\approx$ 4号 $\ge$ 3号

$E$类APP：0号 $\ge$ 2号 $\ge$ 1号 $\approx$ 4号 $\ge$ 3号

$F$类APP：4号 $\ge$ 2号 $\ge$ 1号 $\approx$ 3号 $\ge$ 0号

$G$类APP：2号 $\ge$ 0号 $\ge$ 1号 $\approx$ 4号 $\ge$ 3号

$H$类APP：2号 $\ge$ 0号 $\ge$ 1号 $\approx$ 4号 $\ge$ 3号

$I$类APP：2号 $\ge$ 0号 $\ge$ 4号 $\approx$ 1号 $\ge$ 3号

$J$类APP：2号 $\ge$ 0号 $\ge$ 4号 $\approx$ 1号 $\ge$ 3号

$K$类APP：2号 $\approx$ 0号 $\approx$ 4号 $\approx$ 1号 $\ge$ 3号

$L$类APP：2号 $\approx$ 0号 $\approx$ 4号 $\approx$ 1号 $\approx$ 3号

$M$类APP：2号 $\approx$ 0号 $\ge$ 4号 $\approx$ 1号 $\approx$ 3号

$N$类APP：2号 $\approx$ 0号 $\ge$ 4号 $\approx$ 1号 $\ge$ 3号

$O$类APP：2号 $\approx$ 0号 $\approx$ 4号 $\approx$ 1号 $\ge$ 3号

$P$类APP：2号 $\approx$ 4号 $\ge$ 0号 $\approx$ 1号 $\ge$ 3号

$Q$类APP：2号 $\approx$ 0号 $\ge$ 4号 $\approx$ 1号 $\ge$ 3号

$R$类APP：2号 $\approx$ 0号 $\ge$ 4号 $\approx$ 1号 $\ge$ 3号

$S$类APP：2号 $\approx$ 0号 $\approx$ 4号 $\ge$ 1号 $\ge$ 3号

$T$类APP：2号 $\approx$ 0号 $\ge$ 4号 $\approx$ 1号 $\ge$ 3号
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.9\textwidth]{stripplot_duration_cluster}
    \caption{使用时长}
    \label{fig:015}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.9\textwidth]{stripplot_count_cluster}
    \caption{使用频次}
    \label{fig:016}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.9\textwidth]{stripplot_up_flow_cluster}
    \caption{上行流量}
    \label{fig:018}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.9\textwidth]{stripplot_down_flow_cluster}
    \caption{下行流量}
    \label{fig:019}
\end{figure}

就\cref{fig:008}~总体情况而言，4号用户群使用智能手机的频次最高，使用时长较高，可能需要时刻刷新手机；0号用户群使用智能手机的频次最低，时长最短，对手机依赖程度不强；1号、3号用户群使用流量最多；相对于4号而言，3号用户群不经常刷新手机，单次使用APP时间较长。

\begin{table}[!htbp]
    \caption{聚类各簇用户统计描述}\label{tab:008} \centering
    \begin{tabular}{lrrrr}
        \toprule[1.5pt]
        $cluster$ & $count\_sum$ & $duration\_sum$ & $up\_flow\_sum$ & $down\_flow\_sum$ \\
        \midrule[1pt]
        0         & 375.776880   & 43624           & 392653844       & 65333138          \\
        1         & 1177.867654  & 159571          & 5290795162      & 2128662377        \\
        2         & 1342.008467  & 122244          & 1131347214      & 167123031         \\
        3         & 1573.553903  & 232744          & 12630933652     & 8764644593        \\
        4         & 2931.083175  & 196532          & 1621400073      & 235777958         \\
        \bottomrule[1.5pt]
    \end{tabular}
\end{table}

就\cref{fig:020}~各类APP情况而言，1号、3号用户群似乎对C类APP“情有独钟”；F类APP使用情况最为参差，4号使用时长最长，降序依次是2号、3号、1号、0号。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=.99\textwidth]{plot_profile}
    \caption{各类用户群对各类APP的使用时长（均值）}
    \label{fig:020}
\end{figure}

\newpage
\section{问题二：未来使用情况预测}

\subsection{问题分析与流程思路}



XGBoost、ResNet、

调整参数

ARIMA

固定窗

滑动窗

验证

\newpage
\section{公式}

数学建模必然涉及不少数学公式的使用。下面简单介绍一个可能用得上的数学环境。

首先是行内公式，例如 $ \theta $ 是角度。行内公式使用 \verb|$  $| 包裹。

行间公式不需要编号的可以使用 \verb|\[  \]| 包裹，例如
\[
    E=mc^2
\]
其中 $ E $ 是能量，$ m $ 是质量，$ c $ 是光速。

如果希望某个公式带编号，并且在后文中引用可以参考下面的写法：
\begin{equation}
    E=mc^2
    \label{eq:energy}
\end{equation}
式\cref{eq:energy}是质能方程。

多行公式有时候希望能够在特定的位置对齐，以下是其中一种处理方法。
\begin{align}
    P & = UI   \\
      & = I^2R
\end{align}
\verb|&| 是对齐的位置， \verb|&| 可以有多个，但是每行的个数要相同。

矩阵的输入也不难。
\[
    \mathbf{X} = \left(
    \begin{array}{cccc}
            x_{11} & x_{12} & \ldots & x_{1n} \\
            x_{21} & x_{22} & \ldots & x_{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            x_{n1} & x_{n2} & \ldots & x_{nn} \\
        \end{array} \right)
\]

分段函数这些可以用 \verb|case| 环境，但是它要放在数学环境里面。
\[
    f(x) =
    \begin{cases}
        0 & x \text{为无理数} , \\
        1 & x \text{为有理数} .
    \end{cases}
\]
在数学环境里面，字体用的是数学字体，一般与正文字体不同。假如要公式里面有个别文字，则需要把这部分放在 \verb|text| 环境里面，即 \verb|\text{文本环境}| 。

公式中个别需要加粗的字母可以用 \verb|$\bm{math symbol}$| 。如 $ \alpha a\bm{\alpha a} $ 。

以上仅简单介绍了基础的使用，对于更复杂的需求，可以阅读相关的宏包手册，如 \href{http://texdoc.net/texmf-dist/doc/latex/amsmath/amsldoc.pdf}{amsmath}。

希腊字母这些如果不熟悉，可以去查找符号文件 \href{http://mirrors.ctan.org/info/symbols/comprehensive/symbols-a4.pdf}{symbols-a4.pdf} ，也可以去 \href{http://detexify.kirelabs.org/classify.html}{detexify} 网站手写识别。另外还有数学公式识别软件 \href{https://mathpix.com/}{mathpix} 。

下面简单介绍一下定理、证明等环境的使用。

除了 definition 环境，还可以使用 theorem 、lemma、corollary、assumption、conjecture、axiom、principle、problem、example、proof、solution 这些环境，根据论文的实际需求合理使用。

\begin{theorem}
    这是一个定理。
    \label{thm:example}
\end{theorem}
由\cref{thm:example}我们知道了定理环境的使用。

\begin{lemma}
    这是一个引理。
    \label{lem:example}
\end{lemma}
由\cref{lem:example}我们知道了引理环境的使用。

\begin{corollary}
    这是一个推论。
    \label{cor:example}
\end{corollary}
由\cref{cor:example}我们知道了推论环境的使用。

\begin{assumption}
    这是一个假设。
    \label{asu:example}
\end{assumption}
由\cref{asu:example}我们知道了假设环境的使用。

\begin{conjecture}
    这是一个猜想。
    \label{con:example}
\end{conjecture}
由\cref{con:example}我们知道了猜想环境的使用。

\begin{axiom}
    这是一个公理。
    \label{axi:example}
\end{axiom}
由\cref{axi:example}我们知道了公理环境的使用。

\begin{principle}
    这是一个定律。
    \label{pri:example}
\end{principle}
由\cref{pri:example}我们知道了定律环境的使用。

\begin{problem}
这是一个问题。
\label{pro:example}
\end{problem}
由\cref{pro:example}我们知道了问题环境的使用。

\begin{example}
    这是一个例子。
    \label{exa:example}
\end{example}
由\cref{exa:example}我们知道了例子环境的使用。

\begin{proof}
    这是一个证明。
    \label{prf:example}
\end{proof}
由\cref{prf:example}我们知道了证明环境的使用。

\begin{solution}
    这是一个解。
    \label{sol:example}
\end{solution}
由\cref{sol:example}我们知道了解环境的使用。

%参考文献
\newpage
\begin{thebibliography}{9}
    \bibitem{机器学习} 周志华. 机器学习[M]. 北京. 清华大学出版社. 2016. 197-219
    \bibitem{高维数据的聚类分析} 何宏. 高维数据的聚类分析[M]. 上海. 上海交通大学出版社. 2022. 1-16
    \bibitem{数据仓库与数据挖掘} 陈志泊,韩慧,王建新,孙俏,聂耿青. 数据仓库与数据挖掘[M]. 北京. 清华大学出版社. 2009
    \bibitem{常乐} 常乐. 基于用户行为分析的用户画像系统设计与实现[D]. 北京邮电大学. 2020
    \bibitem{陈纯} 陈纯,龙瀛,黄贵恺. 屏幕使用时间与步行活动关系的探索性研究[J]. 景观设计学(中英文). 2021. 9(04):68-81
    \bibitem{成雪} 成雪,于冬梅,赵丽云等. 2016—2017年中国各省中小学生电子屏幕使用现状[J]. 卫生研究. 2023,52(03):382-387
    \bibitem{武慧娟} 武慧娟,赵天慧,孙鸿飞等. 基于支付意愿的数字阅读用户画像聚类研究[J]. 情报科学. 2022. 40(05)
    \bibitem{侯金凤} 侯金凤. 移动互联网下手机用户使用行为特征的研究[J]. 电脑知识与技术. 2016,12(07)
    \bibitem{韦磊} 韦磊. 基于移动终端数据的用户画像模型研究[D]. 江苏科技大学. 2021
    \bibitem{timeseries} Garg, R., \& Barpanda, S. Machine Learning Algorithms for Time Series Analysis and Forecasting[Z/OL]. arXiv preprint arXiv:2211.14387. https://arxiv.org/abs/2211.14387
    \bibitem{CH} T. Caliński, J Harabasz. A dendrite method for cluster analysis[J]. Communications in Statistics. 1974. 3:1, 1-27
    \bibitem{DB} Davies D L , Bouldin D W. A Cluster Separation Measure[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence. 1979. PAMI-1(2):224-227
    \bibitem{SC} Peter R J . Silhouettes: A graphical aid to the interpretation and validation of cluster analysis[J]. Journal of Computational \& Applied Mathematics. 1987. 20
    \bibitem{sklearn} Swami A , Jain R. Scikit-learn: Machine Learning in Python[J]. Journal of Machine Learning Research. 2013, 12(10):2825-2830
    \bibitem{birch} Zhang T , Ramakrishnan R. Miron Livny: BIRCH: An Efficient Data Clustering Method for Very Large Databases[J]. ACM SIGMOD Record. 1999. 25(2)
    \bibitem{birch2} 赵玉艳,郭景峰,郑丽珍等. 一种改进的BIRCH分层聚类算法[J]. 计算机科学. 2008(03):180-182+208
    \bibitem{eps1} Sander J , Ester M , Kriegel H P ,et al. Density-Based Clustering in Spatial Databases: The Algorithm GDBSCAN and Its Applications[J]. Data Mining \& Knowledge Discovery. 1998. 2(2):169-194
    \bibitem{eps2} Schubert E , Sander J , Ester M ,et al. DBSCAN revisited, revisited: Why and how you should (still) use DBSCAN[J]. ACM Transactions on Database Systems, 2017, 42(3):1-21
    \bibitem{tsne} Maaten L J P V D , Hinton G E. Visualizing High-Dimensional Data using t-SNE[J]. Journal of Machine Learning Research. 2008. 9:2579-2605.
    % \bibitem[6] 论文集： \[序号\] 著者.篇名.主编.论文集名\[C\].出版地.出版者.出版年.起止页码
    % \bibitem[7] 科技报告：\[序号\]著者.题名\[R\]. 报告题名. 编号.出版地.出版者. 出版年.起止页码
    % \bibitem[8] 学位论文：\[序号\] 著者.题名\[D\].保存地点.授予年
    % \bibitem[9] 专利文献：\[序号\] 专利申请者.题名\[P\]. 国别.专利文献种类.专利号.出版日期
\end{thebibliography}
%附录
\newpage
\begin{appendices}

    \section{运行环境与环境依赖}
    代码在Python3.9.12，MacBook Air Apple M1 MacOS13.0.1 (22A400)测试无误。下载解压初赛数据集，将app\_class.csv、dayxx.txt放至Datasets文件夹中，在Visual Studio Code 或 Terminal 中运行 `preprocessing.py`的`generate\_pickle\_origin`，获得pickle格式的粗处理数据，对应2.1、2.2、2.3、2.4小节。

    除数据清洗、预处理源代码直接来自Python脚本，其余附录所列出的代码均由Jupyter Notebook记事本文件导出。
    \begin{lstlisting}[language=Python]
matplotlib~=3.7.2
numpy~=1.23.5
pandas~=2.0.3
scienceplots~=2.1.0
scikit_learn~=1.3.0
seaborn~=0.12.2
tqdm~=4.64.0
xgboost~=1.7.6
ydata_profiling~=4.3.1
    \end{lstlisting}

    \section{数据清洗、预处理源代码}
    \begin{lstlisting}[language=Python]
from tqdm.auto import tqdm
import pandas as pd
import os


def generate_pickle_origin(start_day=1, end_day=21):
    # 读取类型数据
    app = pd.read_csv(
        filepath_or_buffer='../Datasets/app_class.csv',
        header=None
    ).drop_duplicates()

    # 更新列标签
    app.columns = ['appid', 'app_class']

    # 新增一笔虚拟资料（NaN）
    app = pd.concat(
        objs=[pd.DataFrame({"appid": [-404], "app_class": ['NaN']}), app],
        ignore_index=True
    )

    # 约束数据格式
    app['app_class'] = app['app_class'].astype('category')
    app['appid'] = app['appid'].astype('category')

    # 初赛数据集day1～21
    for i in tqdm(range(start_day, end_day + 1)):
        # 读取监测数据
        df = pd.read_csv(
            filepath_or_buffer=f'../Datasets/day{str(i).zfill(2)}.txt',
            header=None
        ).drop_duplicates()

        # 更新列标签
        df.columns = ['uid', 'appid', 'app_type', 'start_day', 'start_time',
                        'end_day', 'end_time', 'duration', 'up_flow', 'down_flow']

        # 新增app_class，对于所属类别未知的APP，类别记作NaN
        df = df.merge(app, on='appid', how='left')
        df['app_class'] = df['app_class'].fillna('NaN')

        # 约束数据格式
        df['appid'] = df['appid'].astype('category')
        df['uid'] = df['uid'].astype('category')
        df['app_type'] = df['app_type'].astype('category')

        # df['start_time'] = pd.to_datetime(df['start_time'], format="%H:%M:%S")
        # df['end_time'] = pd.to_datetime(df['end_time'], format="%H:%M:%S")
        # 使用 pandas.Timestamp 将 `day` 和 `time` 合并
        # df['start_time_new'] = df.apply(
        #     lambda x: x['start_time'] + pd.Timedelta(x['start_day'] - 1, unit='D'), axis=1)
        # df['end_time_new'] = df.apply(
        #     lambda x: x['end_time'] + pd.Timedelta(x['end_day'] - 1, unit='D'), axis=1)

        df = df[['uid', 'appid', 'app_type', 'app_class', 'start_day', 'start_time',
                    'end_day', 'end_time', 'duration', 'up_flow', 'down_flow']]

        # 使用 pickle 存储
        df.to_pickle(f'../Datasets/day{str(i).zfill(2)}.pkl')


def generate_pickle_1(start_day=1, end_day=21):
    """
    数据融合：day01~day21
    建立特征：连接辅助表，新增app_class
    清洗数据：异常值、缺失值、重复值
    """

    # 读取类型数据
    app = pd.read_csv(
        filepath_or_buffer='../Datasets/app_class.csv',
        header=None
    ).drop_duplicates()

    # 更新列标签
    app.columns = ['appid', 'app_class']

    # 新增一笔虚拟资料（NaN）
    app = pd.concat(
        objs=[pd.DataFrame({"appid": [-404], "app_class": ['NaN']}), app],
        ignore_index=True
    )

    # 约束数据格式
    app['app_class'] = app['app_class'].astype('category')
    app['appid'] = app['appid'].astype('category')

    # 初赛数据集day1～21
    for i in tqdm(range(start_day, end_day + 1)):
        # 读取监测数据
        df = pd.read_csv(
            filepath_or_buffer=f'../Datasets/day{str(i).zfill(2)}.txt',
            header=None
        ).drop_duplicates()

        # 更新列标签
        df.columns = ['uid', 'appid', 'app_type', 'start_day', 'start_time',
                        'end_day', 'end_time', 'duration', 'up_flow', 'down_flow']

        # 问题：start_day为负数，甚至持续时间长达一千年
        df = df.query('start_day >=0 & duration <= 9159')

        # 新增app_class，对于所属类别未知的APP，类别记作NaN
        df = df.merge(app, on='appid', how='left')
        df['app_class'] = df['app_class'].fillna('NaN')
        # 问题：app_type 列存在中文
        df['app_class'] = df['app_class'].replace({'用户': 'usr', '预装': 'sys'})

        # 约束数据格式
        df['appid'] = df['appid'].astype('category')
        df['uid'] = df['uid'].astype('category')
        df['app_type'] = df['app_type'].astype('category')

        # df['start_time'] = pd.to_datetime(df['start_time'], format="%H:%M:%S")
        # df['end_time'] = pd.to_datetime(df['end_time'], format="%H:%M:%S")
        # 使用 pandas.Timestamp 将 `day` 和 `time` 合并
        # df['start_time_new'] = df.apply(
        #     lambda x: x['start_time'] + pd.Timedelta(x['start_day'] - 1, unit='D'), axis=1)
        # df['end_time_new'] = df.apply(
        #     lambda x: x['end_time'] + pd.Timedelta(x['end_day'] - 1, unit='D'), axis=1)

        df = df[['uid', 'appid', 'app_type', 'app_class', 'start_day', 'start_time',
                    'end_day', 'end_time', 'duration', 'up_flow', 'down_flow']]

        # 使用 pickle 存储
        df.to_pickle(f'../Datasets/day{str(i).zfill(2)}.pkl')

    \end{lstlisting}

    \section{数据探索：数据概览源代码}

    \begin{lstlisting}[language=Python]
#!/usr/bin/env python
# coding: utf-8
      
# # 载入套件


from typing import Dict
from tqdm.auto import tqdm

import numpy as np
import random
import pandas as pd
import ydata_profiling

import scienceplots
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib

import torch
import os

get_ipython().run_line_magic('matplotlib', 'inline')

pd.plotting.register_matplotlib_converters()
sns.set_style("whitegrid")
sns.set_palette("RdBu")
sns.set(
    rc={'text.usetex': True},
    font="serif",
    font_scale=1.2
)

days = {i: pd.read_pickle(f'../Datasets/day{str(i).zfill(2)}.pkl')
        for i in range(1, 22)}


# # 工具函数


SEED = 20230723


def same_seed(seed=SEED):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


same_seed()


# # 辅助表格

# 辅助表格，常用APP类别
# 发现：原始表格存在重复值
app = pd.read_csv('../Datasets/app_class.csv', header=None).drop_duplicates()
app.columns = ['appid', 'app_class']

# 副本：不锁定数据格式
app_copy = app.copy()

# 约束数据格式
app['appid'] = app['appid'].astype('category')
app['app_class'] = app['app_class'].astype('category')

# 打印行列数、概况
print('app:', app.shape)
pd.DataFrame(app.value_counts('app_class'))
app.describe()



# 21天内监测数据中共有36435种app_id
appid = set({})
for i in range(1, 22):
    appid = appid.union(days[i]['appid'].unique())
len(appid)



# NaN
val = pd.DataFrame({'appid': list(appid)})
val = val.merge(app_copy, on='appid', how='left')
val[val['app_class'].isna()].shape



# a～t
val['app_class'].dropna().shape



sns.catplot(kind='count', data=val.sort_values(by='app_class'),
            x='app_class', height=3.5, aspect=10/3.5)

plt.savefig("../Thesis/figures/app_class_countplot_in_days.pdf",
            dpi=400, bbox_inches='tight', pad_inches=0)



pd.DataFrame(val.fillna('NaN').value_counts(
    ['app_class'])).sort_values(by='count')
 \end{lstlisting}

    \section{数据探索：类别变量代码}

    \begin{lstlisting}[language=Python]
#!/usr/bin/env python
# coding: utf-8

# # 载入套件

# In[1]:


from typing import Dict
from tqdm.auto import tqdm

import numpy as np
import random
import pandas as pd
import ydata_profiling

import scienceplots
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib

import torch
import os

get_ipython().run_line_magic('matplotlib', 'inline')

pd.plotting.register_matplotlib_converters()
sns.set_style("whitegrid")
sns.set_palette("RdBu")
sns.set(
    rc={'text.usetex': True},
    font="serif",
    font_scale=1.2
)

days = {i: pd.read_pickle(f'../Datasets/day{str(i).zfill(2)}.pkl')
        for i in range(1, 22)}


# # 监测表格
# 
# | 变量名        | 释义                          |
# |------------|-----------------------------|
# | uid        | 用户的id                       |
# | appid      | APP的id（与app_class文件中的第一列对应） |
# | app_type   | APP类型：系统自带、用户安装             |
# | start_day  | 使用起始天，取值1-30                |
# | start_time | 使用起始时间                      |
# | end_day    | 使用结束天                       |
# | end_time   | 使用结束时间                      |
# | duration   | 使用时长（秒）                     |
# | up_flow    | 上行流量                        |
# | down_flow  | 下行流量                        |

# # 单一变量

# In[2]:


# 发现：数据集存在中文['用户', '预装']
temp = days[2].select_dtypes('category').describe()
temp


# In[3]:


app_type = set({})
for i in range(1, 22):
    app_type = app_type.union(days[i]['app_type'].unique())
app_type


# In[4]:


for i in range(1, 22):
    days[i].replace({'用户': 'usr', '预装': 'sys'}, inplace=True)


# # 天数变化

# In[5]:


data = {'day': [], 'cat': [], 'val': []}
for i in range(1, 22):
    temp = days[i].select_dtypes('category').describe()
    # 日活跃用户
    data['day'].append(i)
    data['cat'].append('users')
    data['val'].append(temp.loc['unique', 'uid'])

    # 日志
    data['day'].append(i)
    data['cat'].append('logs')
    data['val'].append(temp.loc['count', 'uid'])

    # APP
    data['day'].append(i)
    data['cat'].append('apps')
    data['val'].append(temp.loc['unique', 'appid'])
temp = pd.DataFrame(data)


# In[6]:


for cat in ['logs', 'users', 'apps']:
    g = sns.relplot(kind='line',
                    data=temp[temp['cat'] == cat],
                    x='day',
                    y='val',
                    marker='o')
    g.set(
        ylabel=f'val_{cat}',
        xticks=range(1, 22)
    )
    plt.savefig(f'../Thesis/figures/relplot_line_day_val_{cat}.pdf',
                dpi=400,
                bbox_inches='tight',
                pad_inches=0)


# # 变量关联

# In[7]:


# dfs = []
# for i in range(1, 22):
#     dfs.append(days[i][['appid', 'app_type', 'app_class']].drop_duplicates())
# ((pd.concat(dfs, ignore_index=True)).drop_duplicates()
#  ).to_pickle('../Datasets/app.pkl')


# In[8]:


app = pd.read_pickle('../Datasets/app.pkl')
app['appid'] = app['appid'].astype('category')
app.select_dtypes('category').describe()


# In[9]:


app.groupby('appid').count().query('app_class == 2')


# In[10]:


days[1][(days[1]['appid'] == 4) & (days[1]['app_type'] == 'usr')
        & (days[1]['uid'] == '91C16B1FE338DA085CB0B0840D8C6BA5')]


# In[11]:


g = sns.catplot(kind='count', data=app.query("app_class != 'NaN'"), x='app_class',
                hue='app_type', hue_order=['usr', 'sys'],
                height=5, aspect=10/5)
plt.yscale('log')
plt.savefig("../Thesis/figures/catplot_count_app_class.pdf",
            dpi=400, bbox_inches='tight', pad_inches=0.005)


    \end{lstlisting}

    \section{数据探索：数值变量代码}
    \begin{lstlisting}[language=Python]
#!/usr/bin/env python
# coding: utf-8

# # 载入套件

# In[1]:


from typing import Dict
from tqdm.auto import tqdm

import numpy as np
import random
import pandas as pd
import ydata_profiling

import scienceplots
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib

import torch
import os

get_ipython().run_line_magic('matplotlib', 'inline')

pd.plotting.register_matplotlib_converters()
sns.set_style("whitegrid")
sns.set_palette("RdBu")
sns.set(
    rc={'text.usetex': True},
    font="serif",
    font_scale=1.2
)

days = {i: pd.read_pickle(f'../Datasets/day{str(i).zfill(2)}.pkl')
        for i in range(1, 22)}

# 继承自2.2数据探索之类别变量
for i in range(1, 22):
    days[i]['app_type'] = days[i]['app_type'].replace(
        {'用户': 'usr', '预装': 'sys'})


# # 单一变量

# In[2]:


days[1].select_dtypes('number').describe()


# In[3]:


fig, axes = plt.subplots(1, 3, figsize=(13.5, 5))

for i, x in enumerate(['duration', 'up_flow', 'down_flow']):
    g = sns.boxenplot(data=days[1], y=x, orient='v', ax=axes[i])
    g.set(
        yscale='log'
    )

plt.savefig("../Thesis/figures/boxenplot_duration_day01.pdf",
            dpi=400, bbox_inches='tight', pad_inches=0.005)


# In[4]:


boss = 0
for i in range(1, 22):
    boss = max(boss, days[i]['duration'].quantile(0.9998))
boss


# In[5]:


# days[1].query("uid == '64B3E40461C56847F35DB46D55707EA4'").sort_values(
#     by='end_time')


# In[6]:


# days[1].query("uid == '1F1EF788E30A9EA8E8CB52059A4B02A0'").sort_values(
#     by='end_time')


# In[7]:


days[19].query("app_class == 'a' & duration > 20000")


# In[8]:


# 发现：有‘僵尸用户’（请求/活跃程度极低，监控记录中app_class均为NaN）
days[1][days[1]['uid'] == '3B0AAFBD3213D6DB0CCB17EDEAE80C38']


# In[9]:


# 数据清洗
for i in range(1, 22):
    days[i] = days[i].query('duration <= 9159')


# In[10]:


days[1].select_dtypes('number').describe()


# In[52]:


fig, axes = plt.subplots(1, 5, figsize=(28, 4))
ax = axes[0]
g = sns.histplot(
    data=days[1].pivot_table(
        index='uid',
        values='appid',
        aggfunc='count',
    ),
    ax=ax,
    x='appid'
)


ax = axes[1]
g = sns.histplot(
    data=days[1].pivot_table(
        index='uid',
        values='duration',
        aggfunc=np.sum
    ),
    ax=ax,
    x='duration'
)


ax = axes[2]
g = sns.histplot(
    data=days[1].pivot_table(
        index='uid',
        values='up_flow',
        aggfunc=np.sum
    ),
    ax=ax,
    x='up_flow',
)
g.set(
    xscale='log'
)

ax = axes[3]
g = sns.histplot(
    data=days[1].pivot_table(
        index='uid',
        values='down_flow',
        aggfunc=np.sum
    ),
    ax=ax,
    x='down_flow',
)
g.set(
    xscale='log'
)

ax = axes[4]
g = sns.histplot(
    data=pd.DataFrame(pd.DataFrame(
        days[1].groupby('uid').size()).to_records()),
    ax=ax,
    x='0',
)

plt.savefig("../Thesis/figures/histplots_appid_duration_upflow_downflow_lines.pdf",
            dpi=400, bbox_inches='tight', pad_inches=0.005)    
    \end{lstlisting}

    \section{问题一：量化指标代码}
    \begin{lstlisting}[language=Python]
#!/usr/bin/env python
# coding: utf-8

# # 载入套件

# In[ ]:


from typing import Dict
import numpy as np
import pandas as pd

import ydata_profiling
import scienceplots
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib

get_ipython().run_line_magic('matplotlib', 'inline')

pd.plotting.register_matplotlib_converters()
plt.style.use(['fivethirtyeight', 'science', 'grid'])
sns.set(
    rc={'text.usetex': True},
    font="serif",
    font_scale=1.2
)


# In[ ]:


days = {i: pd.read_pickle(f'../Datasets/day{str(i).zfill(2)}.pkl')
        for i in range(1, 22)}


# # 使用时长

# In[ ]:


dfs = []
value = 'duration'
for i in range(1, 8):
    pt = days[i].pivot_table(
        index='uid',
        columns='app_class',
        values=value,
        aggfunc=np.sum,
        fill_value=0
    )

    feature = pd.DataFrame(pt.to_records())
    feature.columns = [
        f'{value}_{i}' if i != 'uid' else i for i in feature.columns]
    feature.drop(columns=f'{value}_NaN', inplace=True)
    feature['day'] = i
    dfs.append(feature)

feature_duration = pd.concat(dfs, ignore_index=True).groupby([
    'uid']).sum().drop(columns='day')
feature_duration


# # 上行流量

# In[ ]:


dfs = []
value = 'up_flow'
for i in range(1, 8):
    pt = days[i].pivot_table(
        index='uid',
        columns='app_class',
        values=value,
        aggfunc=np.sum,
        fill_value=0
    )

    feature = pd.DataFrame(pt.to_records())
    feature.columns = [
        f'{value}_{i}' if i != 'uid' else i for i in feature.columns]
    feature.drop(columns=f'{value}_NaN', inplace=True)
    feature['day'] = i
    dfs.append(feature)

feature_upflow = pd.concat(dfs, ignore_index=True).groupby([
    'uid']).sum().drop(columns='day')
feature_upflow


# # 下行流量

# In[ ]:


dfs = []
value = 'down_flow'
for i in range(1, 8):
    pt = days[i].pivot_table(
        index='uid',
        columns='app_class',
        values=value,
        aggfunc=np.sum,
        fill_value=0
    )

    feature = pd.DataFrame(pt.to_records())
    feature.columns = [
        f'{value}_{i}' if i != 'uid' else i for i in feature.columns]
    feature.drop(columns=f'{value}_NaN', inplace=True)
    feature['day'] = i
    dfs.append(feature)

feature_downflow = pd.concat(dfs, ignore_index=True).groupby([
    'uid']).sum().drop(columns='day')
feature_downflow


# # 使用频次

# dfs = []
# value = 'count'
# for i in range(1, 8):
#     pt = days[i].pivot_table(
#         index='uid',
#         columns='app_class',
#         values='duration',
#         aggfunc='count',
#         fill_value=0
#     )
# 
#     feature = pd.DataFrame(pt.to_records())
#     feature.columns = [
#         f'{value}_{i}' if i != 'uid' else i for i in feature.columns]
#     feature.drop(columns=f'{value}_NaN', inplace=True)
#     feature['day'] = i
#     dfs.append(feature)
# 
# feature_count = pd.concat(dfs, ignore_index=True).groupby([
#     'uid']).sum().drop(columns='day')
# feature_count
# 

# # 特征融合
# 
# 前7日各类 APP 使用时长、使用频次、上行流量、下行流量

# In[ ]:


features = pd.DataFrame(pd.concat([feature_duration, feature_upflow,
                                    feature_downflow, feature_count], axis=1).to_records())
features.to_pickle('../Datasets/features_q1.pkl')

        
    \end{lstlisting}

    \section{问题一：聚类算法代码}

    \begin{lstlisting}[language=Python]
#!/usr/bin/env python
# coding: utf-8

# # 载入套件

# In[1]:


from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn import metrics
from sklearn.cluster import DBSCAN

from typing import Dict
from tqdm.auto import tqdm

import numpy as np
import random
import pandas as pd
import ydata_profiling

import scienceplots
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
import matplotlib.cm as cm

import torch
import os

get_ipython().run_line_magic('matplotlib', 'inline')

pd.plotting.register_matplotlib_converters()
sns.set_style("whitegrid")
sns.set_palette("RdBu")
sns.set(
    rc={'text.usetex': True},
    font="serif",
    font_scale=1.2
)

SEED = 20230723


def same_seed(seed=SEED):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


same_seed()


# ## 数据预处理

# In[2]:


features = pd.read_pickle('../Datasets/features_q1.pkl')
for aspect in ['duration', 'up_flow', 'down_flow', 'count']:
    cols = [i for i in features.columns if i.startswith(aspect)]
    features[cols] = (features[cols] - features[cols].values.mean()) /         features[cols].values.std()


# ### 皮尔逊相关系数

# In[3]:


# corr = features.drop(columns='uid').corr()
# mask = np.triu(np.ones_like(corr, dtype=bool))
# f, ax = plt.subplots(figsize=(11, 9))
# sns.heatmap(corr, mask=mask, cmap=sns.color_palette("coolwarm"), vmax=.3, center=0,
#             square=True, linewidths=.5, cbar_kws={"shrink": .5})

# plt.savefig("../Thesis/figures/heatmap_features.pdf",
#             dpi=400, bbox_inches='tight', pad_inches=0.005)


# ### 主成分分析

# In[4]:


# pca = PCA()
# pca.fit(features.drop(columns='uid'))
# fs = np.arange(pca.n_components_)[:40]

# plt.figure(figsize=(10, 3))
# g = sns.barplot(x=fs, y=pca.explained_variance_[:40])

# plt.savefig("../Thesis/figures/barplot_pca_features.pdf",
#             dpi=400, bbox_inches='tight', pad_inches=0.005)


# In[5]:


pca = PCA(n_components=3)
pca.fit(features.drop(columns='uid'))
transformed = pca.transform(features.drop(columns='uid'))
X = transformed


# # 聚类分析

# ## 原型聚类：K-Means++

# In[6]:


# Ks = range(2, 52)
# inertias = []
# scores = []

# for k in Ks:
#     model = KMeans(n_clusters=k, random_state=0, n_init="auto", max_iter=1000)
#     model.fit(X)
#     inertias.append(model.inertia_)
#     scores.append(metrics.calinski_harabasz_score(X, model.labels_))

# fig, axes = plt.subplots(1, 2, figsize=(12, 5.5))

# g = sns.lineplot(x=Ks, y=inertias, marker='o', ax=axes[0])
# g.set(
#     xlabel='number of clusters',
#     ylabel='inertia'
# )

# g = sns.lineplot(x=Ks, y=scores,
#                  marker='o', ax=axes[1])
# g.set(
#     xlabel='number of clusters',
#     ylabel='calinski harabasz score'
# )

# plt.savefig("../Thesis/figures/lineplot_Ks_inertias_CH.pdf",
#             dpi=400, bbox_inches='tight', pad_inches=0.005)


# In[7]:


labels_kmeans = KMeans(n_clusters=5, random_state=0,
                        n_init="auto", max_iter=1000).fit_predict(transformed)
labels_kmeans


# In[8]:


labels_kmeans.shape


# ## 密度聚类：DBSCAN

# In[15]:


# from sklearn.neighbors import NearestNeighbors

# # ref: https://stackoverflow.com/questions/15050389/estimating-choosing-optimal-hyperparameters-for-dbscan
# k = 2 * X.shape[-1] - 1
# nbrs = NearestNeighbors(n_neighbors=k, radius=1.0).fit(X)
# distances, indices = nbrs.kneighbors(X)
# distances = np.sort(distances, axis=0)
# distances = distances[:, k-1]

# g = sns.relplot(
#     distances,
#     kind='line'
# )

# plt.axhline(y=2, linestyle=':')

# g.set(
#     xlabel='Index of Users in the dataset',
#     ylabel='Sorted {}-nearest Neighbor Distance'.format(k)
# )

# plt.savefig("../Thesis/figures/relplot_knn_distances.pdf",
#             dpi=400, bbox_inches='tight', pad_inches=0.005)


# In[16]:


db = DBSCAN(eps=2, min_samples=6).fit(X)
labels_dbscan = db.labels_

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)
n_noise_ = list(labels_dbscan).count(-1)

print(f"Estimated number of clusters: {n_clusters_}")
print(f"Estimated number of noise points: {n_noise_}")


# In[17]:


set(labels_dbscan)


# In[18]:


counter = {}
for i in labels_dbscan:
    counter[i] = counter.get(i, 0)+1
counter


# ##  层次聚类：BRICH

# In[11]:


from sklearn.cluster import Birch
import numpy as np
from matplotlib import pyplot as plt
from scipy.cluster.hierarchy import dendrogram
from sklearn.cluster import AgglomerativeClustering

brc = Birch(n_clusters=None)
brc.fit(transformed)
labels_brc = brc.labels_
centroids = brc.subcluster_centers_

model = AgglomerativeClustering(n_clusters=6)
model = model.fit(centroids)


# In[12]:


# def plot_dendrogram(model, **kwargs):
#     # Create linkage matrix and then plot the dendrogram

#     # create the counts of samples under each node
#     counts = np.zeros(model.children_.shape[0])
#     n_samples = len(model.labels_)
#     for i, merge in enumerate(model.children_):
#         current_count = 0
#         for child_idx in merge:
#             if child_idx < n_samples:
#                 current_count += 1  # leaf node
#             else:
#                 current_count += counts[child_idx - n_samples]
#         counts[i] = current_count

#     linkage_matrix = np.column_stack(
#         [model.children_, model.distances_, counts]
#     ).astype(float)

#     # Plot the corresponding dendrogram
#     dendrogram(linkage_matrix, **kwargs)


# plt.figure(figsize=(15, 15))
# plt.title("Hierarchical Clustering Dendrogram")
# # plot the top p levels of the dendrogram
# plot_dendrogram(model, truncate_mode="level", p=6)
# plt.xlabel("Number of points in node")
# plt.xticks(rotation=90)
# plt.savefig("../Thesis/figures/dendrogram.pdf",
#             dpi=400, bbox_inches='tight', pad_inches=0.005)


# In[13]:


labels_brc_agn = []
for i in labels_brc:
    labels_brc_agn.append(model.labels_[i])


# In[14]:


set(labels_brc_agn)


# # 算法比较

# ## 内部评价指标

# In[17]:


data = {
    'Method': ['K-Means++', 'DBSCAN', 'BIRCH+AGNES'],
    'Calinski-Harabasz': [],
    'Davies-Bouldin': [],
    'Silhouette': []
}

for labels in [labels_kmeans, labels_dbscan, labels_brc_agn]:
    data['Calinski-Harabasz'].append(
        metrics.calinski_harabasz_score(transformed, labels))
    data['Davies-Bouldin'].append(
        metrics.davies_bouldin_score(transformed, labels)
    )
    data['Silhouette'].append(
        metrics.silhouette_score(transformed, labels)
    )

df = pd.DataFrame(data)


# In[ ]:


print(df.to_latex())


# ## 3D

# In[ ]:


fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(projection='3d')

ys = transformed[:, 0]
xs = transformed[:, 1]
zs = transformed[:, 2]
ax.scatter(xs, ys, zs,
            color=[f'C{i+1}' for i in labels_brc_agn])

ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

plt.savefig("../Thesis/figures/3d_brc_agn.png",
            dpi=500, bbox_inches='tight', pad_inches=0.005)


# ## t-SNE

# In[20]:


# from sklearn.manifold import TSNE

# # 大概需要运行5分钟
# X_embedded = TSNE(n_components=2, learning_rate='auto',
#                   random_state=0).fit_transform(transformed)
# np.save('t_SNE_X.npy', X_embedded)

X_embedded = np.load('../Datasets/t_SNE_X.npy')
x, y = X_embedded[:, 0], X_embedded[:, 1]


# In[22]:


label = labels_dbscan
plt.figure(figsize=(6, 6))
sns.scatterplot(
    x=x,
    y=y,
    hue=label,
    size=0.2,
    style=label,
    alpha=0.75,
)
plt.legend([], [], frameon=False)
plt.savefig("../Thesis/figures/2d_dbscan.png",
            dpi=200, bbox_inches='tight', pad_inches=0.005)


# In[ ]:




        
    \end{lstlisting}

    \section{问题一：用户画像代码}

    \begin{lstlisting}[language=Python]
#!/usr/bin/env python
# coding: utf-8

# # 载入套件

# In[1]:


from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn import metrics
from sklearn.cluster import DBSCAN

from typing import Dict
from tqdm.auto import tqdm

import numpy as np
import random
import pandas as pd
import ydata_profiling

import scienceplots
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
import matplotlib.cm as cm

import torch
import os

get_ipython().run_line_magic('matplotlib', 'inline')

pd.plotting.register_matplotlib_converters()
sns.set_style("whitegrid")
sns.set_palette("RdBu")
sns.set(
    rc={'text.usetex': True},
    font="serif",
    font_scale=1.2
)

SEED = 20230723


def same_seed(seed=SEED):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


same_seed()

features = pd.read_pickle('../Datasets/features_q1.pkl')
for aspect in ['duration', 'up_flow', 'down_flow', 'count']:
    cols = [i for i in features.columns if i.startswith(aspect)]
    features[cols] = (features[cols] - features[cols].values.mean()) /         features[cols].values.std()

pca = PCA(n_components=3)
pca.fit(features.drop(columns='uid'))
transformed = pca.transform(features.drop(columns='uid'))
X = transformed

model = KMeans(n_clusters=5, random_state=0,
               n_init="auto", max_iter=1000)
labels_kmeans = model.fit_predict(transformed)
labels_kmeans


# In[2]:


features = pd.read_pickle('../Datasets/features_q1.pkl')
features['cluster'] = labels_kmeans
features['cluster'] = features['cluster'].apply(lambda x: str(x))
features


# # 可视化

# In[3]:


fig, axes = plt.subplots(4, 5, figsize=(35, 25))
aspect = 'duration'

for ax, y in zip(axes.flatten(), [i for i in features.columns if i.startswith(aspect)]):
    sns.stripplot(
        ax=ax,
        data=features,
        x='cluster',
        y=y,
        hue="cluster",
        size=1.2,
        alpha=0.95
    )

    ax.legend([], [], frameon=False)

plt.savefig("../Thesis/figures/stripplot_duration_cluster.png",
            dpi=200, bbox_inches='tight', pad_inches=0.005)


# In[4]:


fig, axes = plt.subplots(4, 5, figsize=(35, 25))
aspect = 'count'

for ax, y in zip(axes.flatten(), [i for i in features.columns if i.startswith(aspect)]):
    sns.stripplot(
        ax=ax,
        data=features,
        x='cluster',
        y=y,
        hue="cluster",
        size=1.2,
        alpha=0.95
    )

    ax.legend([], [], frameon=False)

plt.savefig("../Thesis/figures/stripplot_count_cluster.png",
            dpi=200, bbox_inches='tight', pad_inches=0.005)


# In[5]:


fig, axes = plt.subplots(4, 5, figsize=(35, 25))
aspect = 'up_flow'

for ax, y in zip(axes.flatten(), [i for i in features.columns if i.startswith(aspect)]):
    sns.stripplot(
        ax=ax,
        data=features,
        x='cluster',
        y=y,
        hue="cluster",
        size=1.2,
        alpha=0.95
    )

    ax.legend([], [], frameon=False)

plt.savefig("../Thesis/figures/stripplot_up_flow_cluster.png",
            dpi=200, bbox_inches='tight', pad_inches=0.005)


# In[6]:


fig, axes = plt.subplots(4, 5, figsize=(35, 25))
aspect = 'down_flow'

for ax, y in zip(axes.flatten(), [i for i in features.columns if i.startswith(aspect)]):
    sns.stripplot(
        ax=ax,
        data=features,
        x='cluster',
        y=y,
        hue="cluster",
        size=1.2,
        alpha=0.95
    )

    ax.legend([], [], frameon=False)

plt.savefig("../Thesis/figures/stripplot_down_flow_cluster.png",
            dpi=200, bbox_inches='tight', pad_inches=0.005)


# # 数值

# In[7]:


features[['cluster']].value_counts()


# In[8]:


features.shape


# In[9]:


for aspect in ['duration', 'count', 'up_flow', 'down_flow']:
    features[f'{aspect}_sum'] = 0
    for i in [i for i in features.columns if i.startswith(aspect)]:
        features[f'{aspect}_sum'] += features[i]


# In[10]:


pt = features.drop(columns='uid').pivot_table(
    index='cluster',
    aggfunc=np.mean
)

c = 'a'
pt[[f'count_{c}', f'duration_{c}', f'up_flow_{c}', f'down_flow_{c}']]


# In[11]:


x = range(20)
plt.figure(figsize=(10, 5))
for i in range(5):
    y = pt[[i for i in features.columns if i.startswith(
        'duration') and 'sum' not in str(i)]].iloc[i]
    plt.plot(x, y, c=f'C{i}', label=i, alpha=0.75)
plt.xticks(range(20), list("abcdefghijklmnopqrst"))
plt.legend()
plt.savefig("../Thesis/figures/plot_profile.pdf",
            dpi=200, bbox_inches='tight', pad_inches=0.005)
    \end{lstlisting}

\end{appendices}

\end{document}